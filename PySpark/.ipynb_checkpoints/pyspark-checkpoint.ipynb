{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ab6ee8-deb8-4450-ad9c-4bb7cbcfc1ca",
   "metadata": {},
   "source": [
    "**Step 1:** Install Java, Spark, and PySpark\n",
    "PySpark requires Java and Apache Spark.\n",
    "\n",
    "- **1.1 Install Java**\n",
    "- PySpark requires Java 8 or 11.\n",
    "- Check if Java is installed:\n",
    "- Open Command Prompt (cmd) and type:\n",
    "- java -version\n",
    "- If Java is not installed, download and install it from Oracle JDK.\n",
    "- After installation, set the JAVA_HOME environment variable:\n",
    "- Open Control Panel → System → Advanced System Settings → Environment Variables.\n",
    "- Under System Variables, click New and set:\n",
    "- Variable Name: JAVA_HOME\n",
    "- Variable Value: C:\\Program Files\\Java\\jdk-11 (Replace with your installed Java path)\n",
    "- Click OK and restart your PC.\n",
    "\n",
    "- **1.2 Install Apache Spark**\n",
    "- Download Spark from Apache Spark Download.\n",
    "- Select Spark 3.x with Hadoop 3.x.\n",
    "- Choose Pre-built for Apache Hadoop.\n",
    "- Extract the downloaded file to C:\\spark.\n",
    "- Set Environment Variables for Spark:\n",
    "- Open Control Panel → System → Advanced System Settings → Environment Variables.\n",
    "- Under System Variables, click New and set:\n",
    "- Variable Name: SPARK_HOME\n",
    "- Variable Value: C:\\spark (path to your Spark folder)\n",
    "- Find Path in System Variables, click Edit, and add:\n",
    "- C:\\spark\\bin\n",
    "- C:\\spark\\sbin\n",
    "- Click OK and restart your PC.\n",
    "- Verify Spark Installation: Open Command Prompt (cmd) and run:\n",
    "- spark-shell\n",
    "- If Spark starts without errors, it's installed correctly.\n",
    "\n",
    "- **1.3 Install PySpark**\n",
    "- Now, install PySpark using pip:\n",
    "- pip install pyspark\n",
    "- Verify the installation:\n",
    "- pyspark\n",
    "- If PySpark starts, your setup is working.\n",
    "\n",
    "**Step 2:** Install Jupyter Notebook and Configure PySpark\n",
    "- **2.1 Install Jupyter and Findspark**\n",
    "- pip install jupyter findspark\n",
    "- 2.2 Configure PySpark in Jupyter\n",
    "- Create a new Jupyter kernel for PySpark:\n",
    "- python -m ipykernel install --user --name=pyspark --display-name \"Python (PySpark)\"\n",
    "- Start Jupyter Notebook:\n",
    "- jupyter notebook\n",
    "- Open a new notebook and run the following code to initialize PySpark:\n",
    "- import findspark\n",
    "- findspark.init()\n",
    "- from pyspark.sql import SparkSession\n",
    "- spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "- print(spark)\n",
    "- Run the cell. If Spark initializes successfully, your setup is complete!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873dd333-dfdb-40b0-b8b3-4fe0be6fe440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001917BFADB50>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0d8c0-456e-4f1d-9e45-97bd27f3c88b",
   "metadata": {},
   "source": [
    "# Pyspark foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cecfa-38b5-4c64-b16a-df802d1133e0",
   "metadata": {},
   "source": [
    "Apache Spark is a powerful open-source distributed computing system, and PySpark is the Python API for Apache Spark, allowing data scientists and engineers to work with large datasets efficiently.\n",
    "\n",
    "Let’s go step by step through each topic with explanations, why it is important, use cases, and real-time examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc93768-8904-4426-a0d7-775ed14f7c39",
   "metadata": {},
   "source": [
    "### PySpark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75929e-832d-4f7d-9e6e-600b37f74448",
   "metadata": {},
   "source": [
    "PySpark is the Python interface for Apache Spark, which enables big data processing using Python. It allows us to work with Spark’s features, such as distributed computing, real-time stream processing, and machine learning, without writing Java or Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350b0d9-eb49-4fb0-bafe-e714496504b4",
   "metadata": {},
   "source": [
    "### Why is PySpark needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44900639-1c1a-4b69-b695-bf0815f9c2e7",
   "metadata": {},
   "source": [
    "- Traditional data processing tools (like Pandas) cannot handle very large datasets efficiently.\n",
    "\n",
    "- PySpark allows processing terabytes or petabytes of data across multiple nodes.\n",
    "\n",
    "- It integrates well with Hadoop, AWS, and cloud storage solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aad4a0-dbd6-4fc5-871b-92b9bb3887b1",
   "metadata": {},
   "source": [
    "### Use Case in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13ac2d-427d-4c1a-8f2a-5aa7f12acf7f",
   "metadata": {},
   "source": [
    "- Used in ETL (Extract, Transform, Load) processes for cleaning and processing large datasets.\n",
    "\n",
    "- Helpful for handling structured and unstructured data in industries like finance, healthcare, and e-commerce.\n",
    "\n",
    "- Used in Machine Learning Pipelines to train models on large datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fee0c-629d-4e2d-a2ad-9a97ef75c943",
   "metadata": {},
   "source": [
    "**Real-time Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b44743-54b3-4558-84e8-2f8a56db05f2",
   "metadata": {},
   "source": [
    "Netflix’s recommendation system processes petabytes of user data using PySpark to analyze user preferences and suggest movies/shows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed47e8a-ac3f-4312-a5a8-61bf75f9e317",
   "metadata": {},
   "source": [
    "### What is Spark Configuration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7dfab-4d24-46af-a035-aa53bdd6bd18",
   "metadata": {},
   "source": [
    "Spark Configuration refers to how we set up and optimize the Spark environment for distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2162f5-3e6a-48a3-9a4c-ea8e1e452d37",
   "metadata": {},
   "source": [
    "- Configuring Spark properly ensures efficient resource utilization.\n",
    "\n",
    "- Helps in tuning memory usage and execution speed.\n",
    "\n",
    "- Avoids performance bottlenecks when handling large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98ecd53d-b8e7-4915-aa23-60ffbfadc17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001917BFADB50>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyPySparkApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8c169-63dd-4e6c-9a95-57b67dc1af02",
   "metadata": {},
   "source": [
    "- appName → Name of the Spark application.\n",
    "\n",
    "- executor.memory → Allocates memory for each executor (worker node).\n",
    "\n",
    "- driver.memory → Allocates memory for the driver (master node)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c49a7-e534-489d-a2cd-5d906e234715",
   "metadata": {},
   "source": [
    "### Use Case in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d680329-96f8-4c5a-aa43-9f191cd4d07f",
   "metadata": {},
   "source": [
    "- Used when handling large machine learning datasets to optimize memory.\n",
    "\n",
    "- Helps in scaling applications for big data processing efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad19f9-d934-4e80-9bfe-74f8bcdd1b79",
   "metadata": {},
   "source": [
    "A retail company analyzing customer purchase history may need to configure Spark to process millions of transactions per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8f192-eadc-49a3-8d64-bda4b42f64d8",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDD) in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8c0a4-5d69-428a-a420-66fb0893c09c",
   "metadata": {},
   "source": [
    "RDDs (Resilient Distributed Datasets) are the fundamental data structure in Apache Spark. They provide a fault-tolerant, parallel processing, and distributed approach to handle large-scale datasets.\n",
    "\n",
    "In data science, RDDs are useful when working with large datasets that do not fit into memory and need to be processed efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d29e6-d3de-4976-8b67-13ac0853b9e3",
   "metadata": {},
   "source": [
    "### Understanding RDDs in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa5f33-ecbf-4d7c-a583-db81e5c9cfe7",
   "metadata": {},
   "source": [
    "**Resilient** → RDDs recover automatically from failures.\n",
    "\n",
    "**Distributed** → Data is split across multiple nodes for parallel processing.\n",
    "\n",
    "**Dataset** → Collection of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff9466-f820-417d-9637-c3ce411915ca",
   "metadata": {},
   "source": [
    "### Why use RDDs in Data Science?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376df888-ca9e-4a8c-b7bd-37864db348b0",
   "metadata": {},
   "source": [
    "- **Handles Large Data** → Can process TBs of data efficiently.\n",
    "\n",
    "- **Lazy Evaluation** → Executes transformations only when needed.\n",
    "\n",
    "- **In-Memory Computation** → Faster than traditional MapReduce.\n",
    "\n",
    "- **Supports Parallel Processing** → Works on multiple CPU cores.\n",
    "\n",
    "- **Fault Tolerance** → Recovers lost data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bda95-4780-4920-a610-9f3d3dbb8c27",
   "metadata": {},
   "source": [
    "### Creating RDDs in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1ab83-5dd4-4325-871f-90f9ef16e3e8",
   "metadata": {},
   "source": [
    "To work with RDDs, we need PySpark. Let’s start by setting up Spark in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753e45f-b383-4dae-ab27-dd940600ca35",
   "metadata": {},
   "source": [
    "### setup pyspark in jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e80a121-b016-46c5-88a0-669e5952b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # Get Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4082d00d-cef6-4da8-bfa7-7e09bc015735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://OA-STUDENT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MyApp>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5052bb8a-63e5-41c1-9262-ccab38cebb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#findspark.init() → Helps Jupyter Notebook find the PySpark installation.\n",
    "\n",
    "#SparkSession.builder.appName(\"RDDExample\").getOrCreate() → Creates a Spark session.\n",
    "\n",
    "#sc = spark.sparkContext → Creates a Spark Context (needed to work with RDDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb4dfe-bd94-4c47-b611-b700fe57fb35",
   "metadata": {},
   "source": [
    "### Create an RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec83a9-4b34-4960-8681-6e0c3991ffc9",
   "metadata": {},
   "source": [
    "RDDs can be created in two ways:\n",
    "\n",
    "- From a Python list\n",
    "\n",
    "- From an external data source (file, database, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebd6cd-0f88-4b4f-9390-aefc4114e5e6",
   "metadata": {},
   "source": [
    "**Method 1** Create RDD from a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757aaa0c-5b98-40e2-ba3b-48c2ffde9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data \n",
    "data =[10,20,30,40,50]\n",
    "\n",
    "#convert list to rdd\n",
    "rdd=sc.parallelize(data)\n",
    "\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68939b-4bb3-43bb-8fab-76a436fc894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.parallelize(data) → Converts the Python list into an RDD.\n",
    "\n",
    "#collect() → Retrieves all elements from the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff366543-76d8-4b66-8551-7399a6c01bdb",
   "metadata": {},
   "source": [
    " **Method 2:** Create RDD from a Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a3455-95b1-47e9-a1b0-c571ae91c5eb",
   "metadata": {},
   "source": [
    "Let's say we have a file \"data.txt\" with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0c7d980-8b0f-40fd-83c1-653e722bf4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'orange', 'grape', 'mango']\n"
     ]
    }
   ],
   "source": [
    "rdd_file=sc.textFile(\"data.txt\")\n",
    "print(rdd_file.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a6d641dd-e935-4958-bdf1-8c3403babbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.textFile(\"data.txt\") → Reads a text file and creates an RDD.\n",
    "\n",
    "#collect() → Retrieves all lines from the file as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c556c-995e-4360-a23b-9300ac04087c",
   "metadata": {},
   "source": [
    "### RDD Transformations (Processing Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a49027-c6ac-40b3-8027-b8750f29eb20",
   "metadata": {},
   "source": [
    "RDD transformations create new RDDs from existing ones.\n",
    "They are lazy, meaning they are not executed until an action is triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133b2ad-2d28-4a17-83d7-6a4655c812f9",
   "metadata": {},
   "source": [
    " **Example 1:** Map (Apply Function to Each Element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd5f0761-2ec8-42be-8e31-78516bdf02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eacf179d-521a-4f72-a130-38fc37494c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8dbb624c-69c5-49e8-aac6-5af425681928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://OA-STUDENT:4040\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.uiWebUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3cc62f2d-27e2-436b-85b1-00b5e1fd9d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.persist()\n",
    "print(rdd.collect())  # Trigger execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "80dd9311-71a8-430e-b3d1-4d55c0334401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "print(rdd.getStorageLevel())  # Output: Serialized, Memory Deserialized 1x Replicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "df55b785-0df6-4a15-85e2-a691fee96c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "print(rdd.getStorageLevel())  # Output: Serialized, Memory Deserialized 1x Replicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3ca45fa2-ec18-4370-b0c2-c76ee93f6cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=MyApp>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1814411c-6f44-4651-85d5-55b25e00cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the existing SparkContext if it exists\n",
    "from pyspark import SparkContext\n",
    "\n",
    "if 'sc' in locals():\n",
    "    sc.stop()\n",
    "\n",
    "# Now create a new SparkContext\n",
    "sc = SparkContext(appName=\"MyApp\", master=\"local[*]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916c3995-8281-4abf-965d-bb39c76e28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\dell 4\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\dell 4\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e40a18-0702-4a9e-bdd5-d24f4a5dce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Stop any existing SparkContext if it's running\n",
    "if 'sc' in locals():\n",
    "    sc.stop()\n",
    "\n",
    "# Create a new SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Now proceed with your RDD operations\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd_squared = rdd.map(lambda x: x ** 2)\n",
    "print(rdd_squared.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158bb9e-bedb-4fbc-90e8-d3abbe1d9876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3154f-37be-4899-9d54-48005d05ff58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6ac03-fd3e-4cd8-bb77-e0460617eefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
